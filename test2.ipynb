{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels: [2 2 0 0 0 0 1 1 0 0 0 1 2 0 0 0 1 2 0 0 0 0 2 0 0 0 1 0 0 2]\n",
      "Placement Potential: 0.5187265917602997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:273: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulation parameters\n",
    "num_providers = 3\n",
    "num_regions = 3\n",
    "min_dcs_per_provider = 2\n",
    "max_dcs_per_provider = 5\n",
    "num_vms_per_dc = 8\n",
    "vm_processing_capability = 1500  # in MIPS\n",
    "vm_num_cpus = 2\n",
    "vm_ram = 4  # in Gb\n",
    "vm_storage_capacity = 8  # in Gb\n",
    "\n",
    "# Pricing information\n",
    "provider_pricing = {\n",
    "    'Provider 1': {\n",
    "        'US': [0.020, 0.006, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.025, 0.006, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.027, 0.0066, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "    'Provider 2': {\n",
    "        'US': [0.020, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.018, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.020, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "    'Provider 3': {\n",
    "        'US': [0.0095, 0.00120, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.0090, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.0080, 0.0090, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Simulate data for each provider and region\n",
    "provider_data = []\n",
    "for provider_id in range(1, num_providers + 1):\n",
    "    num_dcs = np.random.randint(min_dcs_per_provider, max_dcs_per_provider + 1)\n",
    "    for region in ['US', 'EU', 'AS']:\n",
    "        for dc_id in range(1, num_dcs + 1):\n",
    "            vm_data = {\n",
    "                'provider_id': provider_id,\n",
    "                'region': region,\n",
    "                'dc_id': dc_id,\n",
    "                'num_vms': num_vms_per_dc,\n",
    "                'vm_processing_capability': vm_processing_capability,\n",
    "                'vm_num_cpus': vm_num_cpus,\n",
    "                'vm_ram': vm_ram,\n",
    "                'vm_storage_capacity': vm_storage_capacity,\n",
    "                'cpu_price': provider_pricing[f'Provider {provider_id}'][region][0],\n",
    "                'storage_price': provider_pricing[f'Provider {provider_id}'][region][1],\n",
    "                'intra_dc_bw_price': provider_pricing[f'Provider {provider_id}'][region][2],\n",
    "                'inter_region_bw_price': provider_pricing[f'Provider {provider_id}'][region][3],\n",
    "                'intra_region_bw_price': provider_pricing[f'Provider {provider_id}'][region][4],\n",
    "                'response_time_slo': np.random.uniform(150, 250),  # Simulated response time SLO\n",
    "                'availability_slo': np.random.uniform(0.9, 1.0),  # Simulated availability SLO\n",
    "                'task_count': np.random.choice([1000, 2000, 3000, 5000, 7000, 10000]),\n",
    "            }\n",
    "            provider_data.append(vm_data)\n",
    "\n",
    "# Assuming 'provider_data' includes task-related features\n",
    "features = [\n",
    "    'cpu_price', 'storage_price', 'intra_dc_bw_price', 'inter_region_bw_price',\n",
    "    'total_cost', 'task_count', 'task_size', 'response_time_slo', 'availability_slo'\n",
    "]\n",
    "data = np.array([[vm.get(feature, 0) for feature in features] for vm in provider_data])\n",
    "\n",
    "# Data Identification Phase: Spectral Clustering\n",
    "n_clusters = 3  # Adjust based on the data\n",
    "\n",
    "# Use Spectral Clustering to identify clusters and correlate with SLA violations\n",
    "spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=5)\n",
    "cluster_labels = spectral.fit_predict(data)\n",
    "\n",
    "# Identify data most likely to cause SLA violations if not replicated\n",
    "# Here, we assume that clusters with higher density represent potential SLA violation areas\n",
    "cluster_density = np.bincount(cluster_labels)\n",
    "most_critical_clusters = np.argsort(cluster_density)[-2:]  # Select the top 2 densest clusters\n",
    "\n",
    "critical_data_indices = []\n",
    "for cluster in most_critical_clusters:\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "    critical_data_indices.extend(cluster_indices)\n",
    "\n",
    "# Critical data information\n",
    "critical_data = data[critical_data_indices]\n",
    "\n",
    "# Replica Placement Phase\n",
    "\n",
    "# Integrate the four main parameters into the fuzzy inference system\n",
    "# (response time, availability, cost, potential for data correlation)\n",
    "response_time = np.arange(0, 1.1, 0.1)\n",
    "availability = np.arange(0, 1.1, 0.1)\n",
    "cost = np.arange(0, 1.1, 0.1)\n",
    "data_correlation = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "# Define membership functions for new parameters\n",
    "response_time_low = fuzz.trimf(response_time, [0, 0, 0.5])\n",
    "response_time_high = fuzz.trimf(response_time, [0.5, 1, 1])\n",
    "\n",
    "availability_low = fuzz.trimf(availability, [0, 0, 0.5])\n",
    "availability_high = fuzz.trimf(availability, [0.5, 1, 1])\n",
    "\n",
    "cost_low = fuzz.trimf(cost, [0, 0, 0.5])\n",
    "cost_high = fuzz.trimf(cost, [0.5, 1, 1])\n",
    "\n",
    "data_correlation_low = fuzz.trimf(data_correlation, [0, 0, 0.5])\n",
    "data_correlation_high = fuzz.trimf(data_correlation, [0.5, 1, 1])\n",
    "\n",
    "# Apply the membership functions to input and output variables\n",
    "data_transfer_time_ratio_high = fuzz.trimf(response_time, [0.6, 1, 1])\n",
    "data_transfer_time_ratio_medium = fuzz.trimf(response_time, [0.3, 0.5, 0.7])\n",
    "data_transfer_time_ratio_low = fuzz.trimf(response_time, [0, 0, 0.3])\n",
    "\n",
    "virtual_machine_load_high = fuzz.trimf(availability, [0.6, 1, 1])\n",
    "virtual_machine_load_medium = fuzz.trimf(availability, [0.3, 0.5, 0.7])\n",
    "virtual_machine_load_low = fuzz.trimf(availability, [0, 0, 0.3])\n",
    "\n",
    "data_availability_nr = fuzz.trimf(cost, [0, 0, 0.5])\n",
    "data_availability_r = fuzz.trimf(cost, [0.5, 1, 1])\n",
    "\n",
    "provider_profit_very_low = fuzz.trimf(data_correlation, [0, 0, 0.2])\n",
    "provider_profit_low = fuzz.trimf(data_correlation, [0.2, 0.4, 0.6])\n",
    "provider_profit_medium = fuzz.trimf(data_correlation, [0.4, 0.6, 0.8])\n",
    "provider_profit_high = fuzz.trimf(data_correlation, [0.6, 0.8, 1])\n",
    "provider_profit_very_high = fuzz.trimf(data_correlation, [0.8, 1, 1])\n",
    "\n",
    "placement_potential_very_low = fuzz.trimf(data_correlation, [0, 0, 0.2])\n",
    "placement_potential_low = fuzz.trimf(data_correlation, [0.2, 0.4, 0.6])\n",
    "placement_potential_medium = fuzz.trimf(data_correlation, [0.4, 0.6, 0.8])\n",
    "placement_potential_high = fuzz.trimf(data_correlation, [0.6, 0.8, 1])\n",
    "placement_potential_very_high = fuzz.trimf(data_correlation, [0.8, 1, 1])\n",
    "\n",
    "# Fuzzy inference rules for new parameters\n",
    "rule6 = np.fmin(np.fmin(data_transfer_time_ratio_low, virtual_machine_load_low), data_availability_nr)\n",
    "rule7 = np.fmin(np.fmin(data_transfer_time_ratio_high, virtual_machine_load_high), data_availability_r)\n",
    "rule8 = np.fmin(np.fmax(data_correlation_low, data_correlation_high), np.fmax(provider_profit_low, placement_potential_medium))\n",
    "\n",
    "# Aggregate the rules for new parameters\n",
    "aggregated = np.fmax(np.fmax(rule6, rule7), rule8)\n",
    "\n",
    "# Defuzzify the aggregated result\n",
    "placement_potential_crisp = fuzz.defuzz(data_correlation, aggregated, 'centroid')\n",
    "\n",
    "# Print Results\n",
    "print(\"Cluster Labels:\", cluster_labels)\n",
    "#print(\"Critical Data Information:\", critical_data)\n",
    "print(\"Placement Potential:\", placement_potential_crisp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'task_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate average response time for each task count\u001b[39;00m\n\u001b[0;32m      2\u001b[0m average_response_times \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_count \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtask_counts\u001b[49m:\n\u001b[0;32m      4\u001b[0m     task_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(data[:, features\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_count\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;241m==\u001b[39m task_count)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m     task_response_times \u001b[38;5;241m=\u001b[39m data[task_indices, features\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_time_slo\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'task_counts' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate average response time for each task count\n",
    "average_response_times = {}\n",
    "for task_count in task_counts:\n",
    "    task_indices = np.where(data[:, features.index('task_count')] == task_count)[0]\n",
    "    task_response_times = data[task_indices, features.index('response_time_slo')]\n",
    "    average_response_time = np.mean(task_response_times)\n",
    "    average_response_times[task_count] = average_response_time\n",
    "\n",
    "# Print average response times\n",
    "print(\"\\nAverage Response Times (in ms) for Each Task Count:\")\n",
    "for task_count, avg_response_time in average_response_times.items():\n",
    "    print(f'Task Count: {task_count}, Average Response Time: {avg_response_time:.2f} ms')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
