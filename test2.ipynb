{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Data Centers: 6, Average Response Time: 179.91 ms\n",
      "Number of Data Centers: 9, Average Response Time: 180.04 ms\n",
      "Number of Data Centers: 12, Average Response Time: 179.99 ms\n",
      "Number of Data Centers: 15, Average Response Time: 179.99 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:273: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels: [1 1 1 ... 0 0 1]\n",
      "Placement Potential: 0.5187265917602997\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulation parameters\n",
    "num_providers = 3\n",
    "num_regions = 3\n",
    "min_dcs_per_provider = 2\n",
    "max_dcs_per_provider = 5\n",
    "num_vms_per_dc = 8\n",
    "vm_processing_capability = 1500  # in MIPS\n",
    "vm_num_cpus = 2\n",
    "vm_ram = 4  # in Gb\n",
    "vm_storage_capacity = 8  # in Gb\n",
    "num_data = 200  # Number of data\n",
    "data_size_range = (300, 1000)  # Data size range in Mb\n",
    "task_size_range = (200, 1000)  # Task size range in MI\n",
    "Re = 0.7  # Provider revenues per task execution ($)\n",
    "C_penalty = 0.0025  # Penalty per violation ($)\n",
    "num_dc_list = [6, 9, 12, 15]\n",
    "\n",
    "inter_region_bw_capacity = 500  # in Mb/s\n",
    "inter_region_bw_delay = 150  # in ms\n",
    "\n",
    "intra_region_bw_capacity = 1000  # in Mb/s\n",
    "intra_region_bw_delay = 50  # in ms\n",
    "\n",
    "intra_dc_bw_capacity = 8000  # in Mb/s\n",
    "intra_dc_bw_delay = 10  # in ms\n",
    "\n",
    "# Pricing information\n",
    "provider_pricing = {\n",
    "    'Provider 1': {\n",
    "        'US': [0.020, 0.006, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.025, 0.006, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.027, 0.0066, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "    'Provider 2': {\n",
    "        'US': [0.020, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.018, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.020, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "    'Provider 3': {\n",
    "        'US': [0.0095, 0.00120, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.0090, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.0080, 0.0090, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "}\n",
    "\n",
    "avg_response_times = {}\n",
    "\n",
    "# Simulate data for each provider and region\n",
    "# Vary the number of data centers\n",
    "# Vary the number of data centers\n",
    "for num_dcs in num_dc_list:\n",
    "    provider_data = []  # Reset provider data for each iteration\n",
    "    for provider_id in range(1, num_providers + 1):\n",
    "        for region in ['US', 'EU', 'AS']:\n",
    "            for dc_id in range(1, num_dcs + 1):                \n",
    "                for data_id in range(1, num_data + 1):\n",
    "                    data_size = np.random.uniform(data_size_range[0], data_size_range[1])\n",
    "                    task_size = np.random.uniform(task_size_range[0], task_size_range[1])\n",
    "                    \n",
    "                    base_response_time = 180\n",
    "                    response_time_variation = np.random.normal(loc=0, scale=10)\n",
    "                    response_time = max(0, base_response_time + response_time_variation)\n",
    "                    \n",
    "                    # Include bandwidth and delay information in the data\n",
    "                    vm_data = {\n",
    "                        'provider_id': provider_id,\n",
    "                        'region': region,\n",
    "                        'dc_id': dc_id,\n",
    "                        'num_vms': num_vms_per_dc,\n",
    "                        'vm_processing_capability': vm_processing_capability,\n",
    "                        'vm_num_cpus': vm_num_cpus,\n",
    "                        'vm_ram': vm_ram,\n",
    "                        'vm_storage_capacity': vm_storage_capacity,\n",
    "                        'cpu_price': provider_pricing[f'Provider {provider_id}'][region][0],\n",
    "                        'storage_price': provider_pricing[f'Provider {provider_id}'][region][1],\n",
    "                        'intra_dc_bw_price': provider_pricing[f'Provider {provider_id}'][region][2],\n",
    "                        'inter_region_bw_price': provider_pricing[f'Provider {provider_id}'][region][3],\n",
    "                        'intra_region_bw_price': provider_pricing[f'Provider {provider_id}'][region][4],\n",
    "                        'response_time_slo': response_time,\n",
    "                        'availability_slo': 0.95,\n",
    "                        'task_count': np.random.choice([1000, 2000, 3000, 5000, 7000, 10000]),\n",
    "                        'data_size': data_size,\n",
    "                        'task_size': task_size,\n",
    "                        'inter_region_bw_capacity': inter_region_bw_capacity,\n",
    "                        'inter_region_bw_delay': inter_region_bw_delay,\n",
    "                        'intra_region_bw_capacity': intra_region_bw_capacity,\n",
    "                        'intra_region_bw_delay': intra_region_bw_delay,\n",
    "                        'intra_dc_bw_capacity': intra_dc_bw_capacity,\n",
    "                        'intra_dc_bw_delay': intra_dc_bw_delay,\n",
    "                    }\n",
    "                    provider_data.append(vm_data)\n",
    "\n",
    "    # 'provider_data' includes task-related features\n",
    "    features = [\n",
    "        'cpu_price', 'storage_price', 'intra_dc_bw_price', 'inter_region_bw_price',\n",
    "        'total_cost', 'task_count', 'data_size', 'task_size',\n",
    "        'response_time_slo', 'availability_slo',\n",
    "        'inter_region_bw_capacity', 'inter_region_bw_delay',\n",
    "        'intra_region_bw_capacity', 'intra_region_bw_delay',\n",
    "        'intra_dc_bw_capacity', 'intra_dc_bw_delay' 'provider_id',\n",
    "    ]\n",
    "    data = np.array([[vm.get(feature, 0) for feature in features] for vm in provider_data])\n",
    "\n",
    "    # Calculate average response time\n",
    "    avg_response_time = np.mean(data[:, features.index('response_time_slo')])\n",
    "    print(f\"Number of Data Centers: {num_dcs}, Average Response Time: {avg_response_time:.2f} ms\")\n",
    "\n",
    "\n",
    "# Data Identification Phase: Spectral Clustering\n",
    "n_clusters = 3  # Adjust based on the data\n",
    "\n",
    "# Use Spectral Clustering to identify clusters and correlate with SLA violations\n",
    "spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=5)\n",
    "cluster_labels = spectral.fit_predict(data)\n",
    "\n",
    "# Identify data most likely to cause SLA violations if not replicated\n",
    "# Here, we assume that clusters with higher density represent potential SLA violation areas\n",
    "cluster_density = np.bincount(cluster_labels)\n",
    "most_critical_clusters = np.argsort(cluster_density)[-2:]  # Select the top 2 densest clusters\n",
    "\n",
    "critical_data_indices = []\n",
    "for cluster in most_critical_clusters:\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "    critical_data_indices.extend(cluster_indices)\n",
    "\n",
    "# Critical data information\n",
    "critical_data = data[critical_data_indices]\n",
    "\n",
    "# Replica Placement Phase\n",
    "\n",
    "# Integrate the four main parameters into the fuzzy inference system\n",
    "# (response time, availability, cost, potential for data correlation)\n",
    "response_time = np.arange(0, 1.1, 0.1)\n",
    "availability = np.arange(0, 1.1, 0.1)\n",
    "cost = np.arange(0, 1.1, 0.1)\n",
    "data_correlation = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "# Define membership functions for new parameters\n",
    "response_time_low = fuzz.trimf(response_time, [0, 0, 0.5])\n",
    "response_time_high = fuzz.trimf(response_time, [0.5, 1, 1])\n",
    "\n",
    "availability_low = fuzz.trimf(availability, [0, 0, 0.5])\n",
    "availability_high = fuzz.trimf(availability, [0.5, 1, 1])\n",
    "\n",
    "cost_low = fuzz.trimf(cost, [0, 0, 0.5])\n",
    "cost_high = fuzz.trimf(cost, [0.5, 1, 1])\n",
    "\n",
    "data_correlation_low = fuzz.trimf(data_correlation, [0, 0, 0.5])\n",
    "data_correlation_high = fuzz.trimf(data_correlation, [0.5, 1, 1])\n",
    "\n",
    "# Apply the membership functions to input and output variables\n",
    "data_transfer_time_ratio_high = fuzz.trimf(response_time, [0.6, 1, 1])\n",
    "data_transfer_time_ratio_medium = fuzz.trimf(response_time, [0.3, 0.5, 0.7])\n",
    "data_transfer_time_ratio_low = fuzz.trimf(response_time, [0, 0, 0.3])\n",
    "\n",
    "virtual_machine_load_high = fuzz.trimf(availability, [0.6, 1, 1])\n",
    "virtual_machine_load_medium = fuzz.trimf(availability, [0.3, 0.5, 0.7])\n",
    "virtual_machine_load_low = fuzz.trimf(availability, [0, 0, 0.3])\n",
    "\n",
    "data_availability_nr = fuzz.trimf(cost, [0, 0, 0.5])\n",
    "data_availability_r = fuzz.trimf(cost, [0.5, 1, 1])\n",
    "\n",
    "provider_profit_very_low = fuzz.trimf(data_correlation, [0, 0, 0.2])\n",
    "provider_profit_low = fuzz.trimf(data_correlation, [0.2, 0.4, 0.6])\n",
    "provider_profit_medium = fuzz.trimf(data_correlation, [0.4, 0.6, 0.8])\n",
    "provider_profit_high = fuzz.trimf(data_correlation, [0.6, 0.8, 1])\n",
    "provider_profit_very_high = fuzz.trimf(data_correlation, [0.8, 1, 1])\n",
    "\n",
    "placement_potential_very_low = fuzz.trimf(data_correlation, [0, 0, 0.2])\n",
    "placement_potential_low = fuzz.trimf(data_correlation, [0.2, 0.4, 0.6])\n",
    "placement_potential_medium = fuzz.trimf(data_correlation, [0.4, 0.6, 0.8])\n",
    "placement_potential_high = fuzz.trimf(data_correlation, [0.6, 0.8, 1])\n",
    "placement_potential_very_high = fuzz.trimf(data_correlation, [0.8, 1, 1])\n",
    "\n",
    "# Fuzzy inference rules for new parameters\n",
    "rule6 = np.fmin(np.fmin(data_transfer_time_ratio_low, virtual_machine_load_low), data_availability_nr)\n",
    "rule7 = np.fmin(np.fmin(data_transfer_time_ratio_high, virtual_machine_load_high), data_availability_r)\n",
    "rule8 = np.fmin(np.fmax(data_correlation_low, data_correlation_high), np.fmax(provider_profit_low, placement_potential_medium))\n",
    "\n",
    "# Aggregate the rules for new parameters\n",
    "aggregated = np.fmax(np.fmax(rule6, rule7), rule8)\n",
    "\n",
    "# Defuzzify the aggregated result\n",
    "placement_potential_crisp = fuzz.defuzz(data_correlation, aggregated, 'centroid')\n",
    "\n",
    "\n",
    "# Calculate the amount of SLA violations for each task count considering replication period and weight\n",
    "sla_violations_count = {}\n",
    "replication_period = 32  # Replication period in tasks\n",
    "weight = 0.8  # Weight for violating tasks\n",
    "threshold_response_time = weight * 180  # Threshold response time for violating tasks\n",
    "\n",
    "# Print Results\n",
    "print(\"Cluster Labels:\", cluster_labels)\n",
    "#print(\"Critical Data Information:\", critical_data)\n",
    "print(\"Placement Potential:\", placement_potential_crisp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Response Time (in ms) for each task:\n",
      "Task Count: 1000, Average Response Time: 180.15 ms\n",
      "Task Count: 2000, Average Response Time: 180.02 ms\n",
      "Task Count: 3000, Average Response Time: 179.72 ms\n",
      "Task Count: 5000, Average Response Time: 180.06 ms\n",
      "Task Count: 7000, Average Response Time: 180.02 ms\n",
      "Task Count: 10000, Average Response Time: 179.98 ms\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average response time for each task count\n",
    "average_response_time = {}\n",
    "\n",
    "for task_count in [1000, 2000, 3000, 5000, 7000, 10000]:\n",
    "    task_response_times = []\n",
    "    for vm in provider_data:\n",
    "        if vm['task_count'] == task_count:\n",
    "            response_time = vm.get('response_time_slo', 0)\n",
    "            task_response_times.append(response_time)\n",
    "\n",
    "    # Calculate the average response time for the current task count\n",
    "    if task_response_times:\n",
    "        average_response_time[task_count] = np.mean(task_response_times)\n",
    "    else:\n",
    "        average_response_time[task_count] = 0\n",
    "\n",
    "# Print the results\n",
    "print(\"Average Response Time (in ms) for each task:\")\n",
    "for task_count, avg_rt in average_response_time.items():\n",
    "    print(f\"Task Count: {task_count}, Average Response Time: {avg_rt:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLA Violations in Terms of Response Time (Per Provider):\n",
      "Task Count: 1000\n",
      "  Provider 1: SLA Violations: 1497\n",
      "  Provider 2: SLA Violations: 1526\n",
      "  Provider 3: SLA Violations: 1455\n",
      "Task Count: 2000\n",
      "  Provider 1: SLA Violations: 1514\n",
      "  Provider 2: SLA Violations: 1457\n",
      "  Provider 3: SLA Violations: 1501\n",
      "Task Count: 3000\n",
      "  Provider 1: SLA Violations: 1514\n",
      "  Provider 2: SLA Violations: 1532\n",
      "  Provider 3: SLA Violations: 1524\n",
      "Task Count: 5000\n",
      "  Provider 1: SLA Violations: 1525\n",
      "  Provider 2: SLA Violations: 1465\n",
      "  Provider 3: SLA Violations: 1535\n",
      "Task Count: 7000\n",
      "  Provider 1: SLA Violations: 1491\n",
      "  Provider 2: SLA Violations: 1552\n",
      "  Provider 3: SLA Violations: 1482\n",
      "Task Count: 10000\n",
      "  Provider 1: SLA Violations: 1457\n",
      "  Provider 2: SLA Violations: 1468\n",
      "  Provider 3: SLA Violations: 1502\n"
     ]
    }
   ],
   "source": [
    "# Calculate the amount of SLA violations for each task count and provider\n",
    "sla_violations_count = {}\n",
    "\n",
    "# Calculate SLA violations count for each task count and provider\n",
    "for task_count in [1000, 2000, 3000, 5000, 7000, 10000]:\n",
    "    sla_violations_count[task_count] = {f'Provider {i}': 0 for i in range(1, num_providers + 1)}\n",
    "    for vm in provider_data:\n",
    "        if vm['task_count'] == task_count and vm['response_time_slo'] > threshold_response_time:\n",
    "            sla_violations_count[task_count][f'Provider {vm[\"provider_id\"]}'] += 1\n",
    "\n",
    "# Print the results\n",
    "print(\"SLA Violations in Terms of Response Time (Per Provider):\")\n",
    "for task_count, violations_per_provider in sla_violations_count.items():\n",
    "    print(f\"Task Count: {task_count}\")\n",
    "    for provider, violations_count in violations_per_provider.items():\n",
    "        print(f\"  {provider}: SLA Violations: {violations_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Count: 1000, Normalized Effective Network Usage: 0.28\n",
      "Task Count: 2000, Normalized Effective Network Usage: 0.14\n",
      "Task Count: 3000, Normalized Effective Network Usage: 0.10\n",
      "Task Count: 5000, Normalized Effective Network Usage: 0.06\n",
      "Task Count: 7000, Normalized Effective Network Usage: 0.04\n",
      "Task Count: 10000, Normalized Effective Network Usage: 0.03\n"
     ]
    }
   ],
   "source": [
    "# Calculate Effective Network Usage for each task count\n",
    "effective_network_usage = {}\n",
    "\n",
    "for task_count in task_counts:\n",
    "    task_indices = np.where(data[:, features.index('task_count')] == task_count)[0]\n",
    "    if len(task_indices) > 0:\n",
    "        total_data_transfer_time = np.sum(\n",
    "            data[task_indices, features.index('data_size')] /\n",
    "            data[task_indices, features.index('inter_region_bw_capacity')] +\n",
    "            data[task_indices, features.index('data_size')] /\n",
    "            data[task_indices, features.index('intra_region_bw_capacity')] +\n",
    "            data[task_indices, features.index('data_size')] /\n",
    "            data[task_indices, features.index('intra_dc_bw_capacity')]\n",
    "        )\n",
    "\n",
    "        # Normalize the Effective Network Usage\n",
    "        effective_network_usage = total_data_transfer_time / (replication_period * task_count) \n",
    "        print(f\"Task Count: {task_count}, Normalized Effective Network Usage: {effective_network_usage:.2f}\")\n",
    "    else:\n",
    "        print(f\"Task Count: {task_count}, No data available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider 1, Average Monetary Profit: $3222.08\n",
      "Provider 2, Average Monetary Profit: $3236.78\n",
      "Provider 3, Average Monetary Profit: $3252.34\n"
     ]
    }
   ],
   "source": [
    "# Calculate average total monetary profit per provider\n",
    "providers = ['Provider 1', 'Provider 2', 'Provider 3']\n",
    "\n",
    "for provider in providers:\n",
    "    provider_indices = np.where(np.array([vm['provider_id'] for vm in provider_data]) == providers.index(provider) + 1)[0]\n",
    "    if len(provider_indices) > 0:\n",
    "        total_sla_violations = np.sum(data[provider_indices, features.index('response_time_slo')] > threshold_response_time)\n",
    "        total_monetary_profit = np.sum(\n",
    "            (data[provider_indices, features.index('total_cost')] * (1 - placement_potential_crisp)) +\n",
    "            (data[provider_indices, features.index('task_count')] * Re) -\n",
    "            (total_sla_violations * C_penalty)\n",
    "        )\n",
    "        average_monetary_profit = total_monetary_profit / len(provider_indices)\n",
    "        print(f\"{provider}, Average Monetary Profit: ${average_monetary_profit:.2f}\")\n",
    "    else:\n",
    "        print(f\"{provider}, No data available.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
