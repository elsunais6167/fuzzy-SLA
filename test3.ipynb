{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (4800) does not match length of index (27000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Ensure that the length of provider_info matches the total number of rows in the DataFrame\u001b[39;00m\n\u001b[0;32m    119\u001b[0m provider_info \u001b[38;5;241m=\u001b[39m provider_info[:\u001b[38;5;28mlen\u001b[39m(df)]\n\u001b[1;32m--> 121\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprovider\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m provider_info\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Print the DataFrame\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4094\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4092\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4093\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4094\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4303\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4295\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4296\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4301\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4303\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4306\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4307\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4308\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4309\u001b[0m     ):\n\u001b[0;32m   4310\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5042\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5042\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (4800) does not match length of index (27000)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulation parameters\n",
    "num_providers = 3\n",
    "num_regions = 3\n",
    "min_dcs_per_provider = 2\n",
    "max_dcs_per_provider = 5\n",
    "num_vms_per_dc = 8\n",
    "vm_processing_capability = 1500  # in MIPS\n",
    "vm_num_cpus = 2\n",
    "vm_ram = 4  # in Gb\n",
    "vm_storage_capacity = 8  # in Gb\n",
    "num_data = 200  # Number of data\n",
    "data_size_range = (300, 1000)  # Data size range in Mb\n",
    "task_size_range = (200, 1000)  # Task size range in MI\n",
    "Re = 0.7  # Provider revenues per task execution ($)\n",
    "C_penalty = 0.0025  # Penalty per violation ($)\n",
    "num_dc_list = [6, 9, 12, 15]\n",
    "\n",
    "inter_region_bw_capacity = 500  # in Mb/s\n",
    "inter_region_bw_delay = 150  # in ms\n",
    "\n",
    "intra_region_bw_capacity = 1000  # in Mb/s\n",
    "intra_region_bw_delay = 50  # in ms\n",
    "\n",
    "intra_dc_bw_capacity = 8000  # in Mb/s\n",
    "intra_dc_bw_delay = 10  # in ms\n",
    "\n",
    "# Pricing information\n",
    "operating_cost = {\n",
    "    'Provider 1': {\n",
    "        'US': [0.020, 0.006, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.025, 0.006, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.027, 0.0066, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "    'Provider 2': {\n",
    "        'US': [0.020, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.018, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.020, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "    'Provider 3': {\n",
    "        'US': [0.0095, 0.00120, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.0090, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.0080, 0.0090, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Simulate data for each provider and region\n",
    "for num_dcs in num_dc_list:\n",
    "    provider_data = []  # Reset provider data for each iteration\n",
    "    for provider_id in range(1, num_providers + 1):\n",
    "        for region in ['US', 'EU', 'AS']:\n",
    "            for dc_id in range(1, num_dcs + 1):                \n",
    "                for data_id in range(1, num_data + 1):\n",
    "                    data_size = np.random.uniform(data_size_range[0], data_size_range[1])\n",
    "                    task_size = np.random.uniform(task_size_range[0], task_size_range[1])\n",
    "                    \n",
    "                    base_response_time = 180\n",
    "                    response_time_variation = np.random.normal(loc=0, scale=10)\n",
    "                    response_time = max(0, base_response_time + response_time_variation)\n",
    "                    \n",
    "                    # Include bandwidth and delay information in the data\n",
    "                    vm_data = {\n",
    "                        'provider_id': provider_id,\n",
    "                        'region': region,\n",
    "                        'dc_id': dc_id,\n",
    "                        'num_vms': num_vms_per_dc,\n",
    "                        'vm_processing_capability': vm_processing_capability,\n",
    "                        'vm_num_cpus': vm_num_cpus,\n",
    "                        'vm_ram': vm_ram,\n",
    "                        'vm_storage_capacity': vm_storage_capacity,\n",
    "                        'cpu_cost': operating_cost[f'Provider {provider_id}'][region][0],\n",
    "                        'storage_cost': operating_cost[f'Provider {provider_id}'][region][1],\n",
    "                        'intra_dc_bw_cost': operating_cost[f'Provider {provider_id}'][region][2],\n",
    "                        'inter_region_bw_cost': operating_cost[f'Provider {provider_id}'][region][3],\n",
    "                        'intra_region_bw_cost': operating_cost[f'Provider {provider_id}'][region][4],\n",
    "                        'response_time_slo': response_time,\n",
    "                        'availability_slo': 0.95,\n",
    "                        'task_count': np.random.choice([1000, 2000, 3000, 5000, 7000, 10000]),\n",
    "                        'data_size': data_size,\n",
    "                        'task_size': task_size,\n",
    "                        'inter_region_bw_capacity': inter_region_bw_capacity,\n",
    "                        'inter_region_bw_delay': inter_region_bw_delay,\n",
    "                        'intra_region_bw_capacity': intra_region_bw_capacity,\n",
    "                        'intra_region_bw_delay': intra_region_bw_delay,\n",
    "                        'intra_dc_bw_capacity': intra_dc_bw_capacity,\n",
    "                        'intra_dc_bw_delay': intra_dc_bw_delay,\n",
    "                    }\n",
    "                    provider_data.append(vm_data)\n",
    "\n",
    "    # 'provider_data' includes task-related features\n",
    "    # 'provider_data' includes task-related features\n",
    "features = [\n",
    "    'cpu_cost', 'storage_cost', 'intra_dc_bw_cost', 'inter_region_bw_cost',\n",
    "    'total_cost', 'task_count', 'data_size', 'task_size', 'intra_region_bw_cost',\n",
    "    'response_time_slo', 'availability_slo',\n",
    "    'inter_region_bw_capacity', 'inter_region_bw_delay',\n",
    "    'intra_region_bw_capacity', 'intra_region_bw_delay',\n",
    "    'intra_dc_bw_capacity', 'intra_dc_bw_delay', 'provider_id',\n",
    "]\n",
    "\n",
    "data = np.array([[vm.get(feature, 0) for feature in features] for vm in provider_data])\n",
    "\n",
    "# Create a DataFrame using Pandas\n",
    "df = pd.DataFrame(data, columns=features)\n",
    "\n",
    "# Add provider information\n",
    "provider_info = []\n",
    "for provider_id in range(1, num_providers + 1):\n",
    "    num_dcs = np.random.randint(min_dcs_per_provider, max_dcs_per_provider + 1)\n",
    "    provider_info.extend([f'Provider {provider_id}'] * (num_regions * num_dcs * num_data))\n",
    "\n",
    "# Ensure that the length of provider_info matches the total number of rows in the DataFrame\n",
    "provider_info = provider_info[:len(df)]\n",
    "\n",
    "df['provider'] = provider_info\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Data Identification Phase: Spectral Clustering\n",
    "n_clusters = 3  # Adjust based on the data\n",
    "\n",
    "# Use Spectral Clustering to identify clusters and correlate with SLA violations\n",
    "spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=5)\n",
    "cluster_labels = spectral.fit_predict(data)\n",
    "\n",
    "# Identify data most likely to cause SLA violations if not replicated\n",
    "# Here, we assume that clusters with higher density represent potential SLA violation areas\n",
    "cluster_density = np.bincount(cluster_labels)\n",
    "most_critical_clusters = np.argsort(cluster_density)[-2:]  # Select the top 2 densest clusters\n",
    "\n",
    "critical_data_indices = []\n",
    "for cluster in most_critical_clusters:\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "    critical_data_indices.extend(cluster_indices)\n",
    "\n",
    "# Critical data information\n",
    "critical_data = data[critical_data_indices]\n",
    "\n",
    "# Assuming you have the critical_data from the last part of your script\n",
    "# Extract necessary information for calculations\n",
    "data_sizes = critical_data[:, features.index('data_size')]\n",
    "data_transfer_times = critical_data[:, features.index('response_time_slo')]\n",
    "\n",
    "# Updated Data Transfer Cost Calculation\n",
    "data_transfer_cost = (\n",
    "    (data_sizes / critical_data[:, features.index('intra_dc_bw_capacity')]) * critical_data[:, features.index('intra_dc_bw_cost')] +\n",
    "    (data_sizes / critical_data[:, features.index('inter_region_bw_capacity')]) * critical_data[:, features.index('inter_region_bw_cost')] +\n",
    "    (data_sizes / critical_data[:, features.index('intra_region_bw_capacity')]) * critical_data[:, features.index('intra_region_bw_cost')]\n",
    ")\n",
    "\n",
    "# 1. Data Transfer Time Ratio\n",
    "longest_data_transfer_time = np.max(data_transfer_times)\n",
    "data_transfer_time_ratios = data_transfer_times / longest_data_transfer_time\n",
    "\n",
    "# 2. Virtual Machine's Load\n",
    "queuing_capacities = np.random.uniform(1, 10, size=len(critical_data))\n",
    "processing_capacities = np.random.uniform(1, 10, size=len(critical_data))\n",
    "vm_loads = queuing_capacities + processing_capacities\n",
    "\n",
    "# 3. Availability\n",
    "MRFD = 2\n",
    "availability_slo = critical_data[:, features.index('availability_slo')]\n",
    "availability = 1 - (1 - 1 / MRFD) ** MRFD\n",
    "\n",
    "# 4. Profit\n",
    "num_tasks_executed = critical_data[:, features.index('task_count')]\n",
    "revenues_per_task_execution = Re\n",
    "task_execution_revenues = num_tasks_executed * revenues_per_task_execution\n",
    "\n",
    "data_transfer_volume = data_sizes / critical_data[:, features.index('intra_dc_bw_capacity')]\n",
    "data_transfer_revenue = data_transfer_cost * data_transfer_volume\n",
    "\n",
    "cpu_cost = critical_data[:, features.index('cpu_cost')]\n",
    "storage_cost = critical_data[:, features.index('storage_cost')]\n",
    "intra_dc_bw_cost = critical_data[:, features.index('intra_dc_bw_cost')]\n",
    "task_execution_cost = num_tasks_executed * (cpu_cost + storage_cost + intra_dc_bw_cost)\n",
    "\n",
    "data_management_cost = data_transfer_cost\n",
    "\n",
    "operating_cost_vm = task_execution_cost + data_management_cost\n",
    "\n",
    "profit_vm = task_execution_revenues - operating_cost_vm\n",
    "\n",
    "# Display Results\n",
    "for i in range(len(critical_data)):\n",
    "    print(f\"Data {i+1} - Data Transfer Time Ratio: {data_transfer_time_ratios[i]:.4f}, VM Load: {vm_loads[i]:.4f}, \"\n",
    "          f\"Availability: {availability[i]:.4f}, Profit: {profit_vm[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
