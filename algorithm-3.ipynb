{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_spectral.py:688: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\n",
      "c:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:273: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 1ms/step\n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "108/108 [==============================] - 0s 2ms/step\n",
      "Placement Decision for Cluster 1: Very Low\n",
      "Placement Decision for Cluster 2: Very Low\n",
      "Placement Decision for Cluster 3: Very Low\n",
      "Number of Executed Tasks: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "class DataProvider:\n",
    "    def __init__(self, name, cpu_prices, storage_prices, bw_prices):\n",
    "        self.name = name\n",
    "        self.cpu_prices = cpu_prices\n",
    "        self.storage_prices = storage_prices\n",
    "        self.bw_prices = bw_prices\n",
    "\n",
    "class Region:\n",
    "    def __init__(self, name, data_provider, num_dcs):\n",
    "        self.name = name\n",
    "        self.data_provider = data_provider\n",
    "        self.num_dcs = num_dcs\n",
    "        self.data_centers = [DataCenter(f\"{name}_DC{i}\", self) for i in range(num_dcs)]\n",
    "\n",
    "class DataCenter:\n",
    "    def __init__(self, name, region):\n",
    "        self.name = name\n",
    "        self.region = region\n",
    "        # Add other attributes like VMs, tasks, etc., as needed\n",
    "\n",
    "class NeuroFuzzyInferenceSystem:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(10, input_dim=self.data.shape[1], activation='relu'),\n",
    "            tf.keras.layers.Dense(5, activation='relu'),\n",
    "            tf.keras.layers.Dense(3, activation='softmax')  # Assuming 3 fuzzy sets (Low, Medium, High)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fuzzy_inference(self, input_data):\n",
    "        fuzzy_sets = self.model.predict(input_data)\n",
    "        # Fuzzy inference logic (centroid defuzzification)\n",
    "        centroid = np.argmax(fuzzy_sets, axis=1)\n",
    "        return centroid\n",
    "\n",
    "    def interpret_decision(self, fuzzy_decision):\n",
    "        # Interpret fuzzy decision based on the placement rules\n",
    "        if fuzzy_decision == 0:\n",
    "            return \"Very Low\"\n",
    "        elif fuzzy_decision == 1:\n",
    "            return \"Low\"\n",
    "        elif fuzzy_decision == 2:\n",
    "            return \"Medium\"\n",
    "        elif fuzzy_decision == 3:\n",
    "            return \"High\"\n",
    "        elif fuzzy_decision == 4:\n",
    "            return \"Very High\"\n",
    "        \n",
    "\n",
    "# Simulation parameters\n",
    "num_providers = 3\n",
    "num_regions = 3\n",
    "num_dcs_min = 2\n",
    "num_dcs_max = 5\n",
    "num_vms_per_dc = 8\n",
    "intra_dc_bw = 8000  # Mb/s\n",
    "vm_processing_capability = 1500  # MIPS\n",
    "vm_cpu = 2\n",
    "vm_ram = 4  # GB\n",
    "vm_storage_capacity = 8  # GB\n",
    "rev_per_task = 0.7  # $\n",
    "penalty_per_violation = 0.0025  # $\n",
    "response_time_slo = 180  # s\n",
    "availability_slo = 0.95\n",
    "replication_period = 32\n",
    "threshold_thrt = 0.8 * response_time_slo\n",
    "num_clusters = 3\n",
    "\n",
    "# Rule parameters\n",
    "data_transfer_time_ratio = [\"High\", \"High\", \"Medium\", \"Medium\", \"Low\"]\n",
    "vm_load = [\"High\", \"Medium\", \"Medium\", \"Medium\", \"Low\"]\n",
    "data_availability = [\"NR\", \"NR\", \"NR\", \"R\", \"R\"]\n",
    "provider_profit = [\"NP\", \"P\", \"P\", \"P\", \"P\"]\n",
    "placement_potential = [\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"]\n",
    "\n",
    "# Create data providers and regions\n",
    "provider1 = DataProvider(\"Provider1\", [0.020, 0.025, 0.027], [0.006, 0.006, 0.0066], [0.001, 0.0015, 0.002])\n",
    "provider2 = DataProvider(\"Provider2\", [0.020, 0.018, 0.020], [0.0096, 0.008, 0.0096], [0.001, 0.0015, 0.002])\n",
    "provider3 = DataProvider(\"Provider3\", [0.0095, 0.0090, 0.0080], [0.00120, 0.0096, 0.0090], [0.001, 0.0015, 0.002])\n",
    "\n",
    "region1 = Region(\"US\", provider1, num_dcs_max)\n",
    "region2 = Region(\"EU\", provider2, num_dcs_min)\n",
    "region3 = Region(\"AS\", provider3, num_dcs_max)\n",
    "\n",
    "# Data identification phase\n",
    "data_identification_data = np.random.rand(num_tasks_max, num_tasks_max)\n",
    "identified_data_labels = SpectralClustering(n_clusters=num_clusters, assign_labels=\"discretize\", random_state=0).fit_predict(data_identification_data)\n",
    "\n",
    "# Replica placement phase\n",
    "replica_placement_data = np.random.rand(num_tasks_max, 4)  # Assuming 4 input parameters for the neuro-fuzzy system\n",
    "nfis = NeuroFuzzyInferenceSystem(replica_placement_data)\n",
    "\n",
    "# Convert the identified data into one-hot encoded labels for training the neuro-fuzzy model\n",
    "identified_data_labels_onehot = tf.keras.utils.to_categorical(identified_data_labels, num_clusters)\n",
    "\n",
    "# Train the neuro-fuzzy model\n",
    "nfis.model.fit(replica_placement_data, identified_data_labels_onehot, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Placeholder for results\n",
    "placement_decisions = []\n",
    "\n",
    "# Simulate replica placement for each identified data\n",
    "for label in set(identified_data_labels):\n",
    "    # Extract data related to the identified cluster\n",
    "    cluster_data = replica_placement_data[identified_data_labels == label]\n",
    "\n",
    "    # Get placement decision from the neuro-fuzzy inference system\n",
    "    fuzzy_decision = nfis.fuzzy_inference(cluster_data)\n",
    "\n",
    "    # Interpret fuzzy decision based on placement rules\n",
    "    interpreted_decision = nfis.interpret_decision(fuzzy_decision[0])\n",
    "\n",
    "    placement_decisions.append(interpreted_decision)\n",
    "\n",
    "# Display placement decisions\n",
    "for i, decision in enumerate(placement_decisions):\n",
    "    print(f\"Placement Decision for Cluster {i+1}: {decision}\")\n",
    "\n",
    "executed_tasks = 0\n",
    "\n",
    "# Simulate task execution for each identified data\n",
    "for label, decision in zip(set(identified_data_labels), placement_decisions):\n",
    "    # Extract data related to the identified cluster\n",
    "    cluster_data = replica_placement_data[identified_data_labels == label]\n",
    "\n",
    "    # Check if the placement decision meets the response time SLO\n",
    "    response_time = np.mean(cluster_data[:, 0])  # Assuming response time is the first parameter\n",
    "    if response_time <= response_time_slo:\n",
    "        # Task execution is successful\n",
    "        executed_tasks += 1\n",
    "\n",
    "# Display the number of executed tasks\n",
    "print(f\"Number of Executed Tasks: {executed_tasks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_spectral.py:688: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\n",
      "c:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:273: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 0s 1ms/step\n",
      "103/103 [==============================] - 0s 1ms/step\n",
      "104/104 [==============================] - 0s 1ms/step\n",
      "Placement Decision for Cluster 1: Low\n",
      "Placement Decision for Cluster 2: Medium\n",
      "Placement Decision for Cluster 3: Medium\n",
      "Accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YANSILIYU\\AppData\\Local\\Temp\\ipykernel_11364\\2344647904.py:140: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  accuracy = np.mean(np.array(placement_decisions) == np.array(identified_data_labels))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "class DataProvider:\n",
    "    def __init__(self, name, cpu_prices, storage_prices, bw_prices):\n",
    "        self.name = name\n",
    "        self.cpu_prices = cpu_prices\n",
    "        self.storage_prices = storage_prices\n",
    "        self.bw_prices = bw_prices\n",
    "\n",
    "class Region:\n",
    "    def __init__(self, name, data_provider, num_dcs):\n",
    "        self.name = name\n",
    "        self.data_provider = data_provider\n",
    "        self.num_dcs = num_dcs\n",
    "        self.data_centers = [DataCenter(f\"{name}_DC{i}\", self) for i in range(num_dcs)]\n",
    "\n",
    "class DataCenter:\n",
    "    def __init__(self, name, region):\n",
    "        self.name = name\n",
    "        self.region = region\n",
    "        # Add other attributes like VMs, tasks, etc., as needed\n",
    "\n",
    "class NeuroFuzzyInferenceSystem:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(10, input_dim=self.data.shape[1], activation='relu'),\n",
    "            tf.keras.layers.Dense(5, activation='relu'),\n",
    "            tf.keras.layers.Dense(3, activation='softmax')  # Assuming 3 fuzzy sets (Low, Medium, High)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fuzzy_inference(self, input_data):\n",
    "        fuzzy_sets = self.model.predict(input_data)\n",
    "        # Fuzzy inference logic (centroid defuzzification)\n",
    "        centroid = np.argmax(fuzzy_sets, axis=1)\n",
    "        return centroid\n",
    "\n",
    "    def interpret_decision(self, fuzzy_decision):\n",
    "        # Interpret fuzzy decision based on the placement rules\n",
    "        if fuzzy_decision == 0:\n",
    "            return \"Very Low\"\n",
    "        elif fuzzy_decision == 1:\n",
    "            return \"Low\"\n",
    "        elif fuzzy_decision == 2:\n",
    "            return \"Medium\"\n",
    "        elif fuzzy_decision == 3:\n",
    "            return \"High\"\n",
    "        elif fuzzy_decision == 4:\n",
    "            return \"Very High\"\n",
    "\n",
    "# Simulation parameters\n",
    "num_providers = 3\n",
    "num_regions = 3\n",
    "num_dcs_min = 2\n",
    "num_dcs_max = 5\n",
    "num_vms_per_dc = 8\n",
    "num_tasks_min = 1000\n",
    "num_tasks_max = 10000\n",
    "task_size_min = 200\n",
    "task_size_max = 1000\n",
    "data_size_min = 300\n",
    "data_size_max = 1000\n",
    "inter_region_bw = 500  # Mb/s\n",
    "intra_region_bw = 1000  # Mb/s\n",
    "intra_dc_bw = 8000  # Mb/s\n",
    "vm_processing_capability = 1500  # MIPS\n",
    "vm_cpu = 2\n",
    "vm_ram = 4  # GB\n",
    "vm_storage_capacity = 8  # GB\n",
    "rev_per_task = 0.7  # $\n",
    "penalty_per_violation = 0.0025  # $\n",
    "response_time_slo = 180  # s\n",
    "availability_slo = 0.95\n",
    "replication_period = 32\n",
    "threshold_thrt = 0.8 * response_time_slo\n",
    "num_clusters = 3\n",
    "\n",
    "# Rule parameters\n",
    "data_transfer_time_ratio = [\"High\", \"High\", \"Medium\", \"Medium\", \"Low\"]\n",
    "vm_load = [\"High\", \"Medium\", \"Medium\", \"Medium\", \"Low\"]\n",
    "data_availability = [\"NR\", \"NR\", \"NR\", \"R\", \"R\"]\n",
    "provider_profit = [\"NP\", \"P\", \"P\", \"P\", \"P\"]\n",
    "placement_potential = [\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"]\n",
    "\n",
    "# Create data providers and regions\n",
    "provider1 = DataProvider(\"Provider1\", [0.020, 0.025, 0.027], [0.006, 0.006, 0.0066], [0.001, 0.0015, 0.002])\n",
    "provider2 = DataProvider(\"Provider2\", [0.020, 0.018, 0.020], [0.0096, 0.008, 0.0096], [0.001, 0.0015, 0.002])\n",
    "provider3 = DataProvider(\"Provider3\", [0.0095, 0.0090, 0.0080], [0.00120, 0.0096, 0.0090], [0.001, 0.0015, 0.002])\n",
    "\n",
    "region1 = Region(\"US\", provider1, num_dcs_max)\n",
    "region2 = Region(\"EU\", provider2, num_dcs_min)\n",
    "region3 = Region(\"AS\", provider3, num_dcs_max)\n",
    "\n",
    "# Data identification phase\n",
    "data_identification_data = np.random.rand(num_tasks_max, num_tasks_max)\n",
    "identified_data_labels = SpectralClustering(n_clusters=num_clusters, assign_labels=\"discretize\", random_state=0).fit_predict(data_identification_data)\n",
    "\n",
    "# Replica placement phase\n",
    "replica_placement_data = np.random.rand(num_tasks_max, 4)  # Assuming 4 input parameters for the neuro-fuzzy system\n",
    "nfis = NeuroFuzzyInferenceSystem(replica_placement_data)\n",
    "\n",
    "# Convert the identified data into one-hot encoded labels for training the neuro-fuzzy model\n",
    "identified_data_labels_onehot = tf.keras.utils.to_categorical(identified_data_labels, num_clusters)\n",
    "\n",
    "# Train the neuro-fuzzy model\n",
    "nfis.model.fit(replica_placement_data, identified_data_labels_onehot, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Placeholder for results\n",
    "placement_decisions = []\n",
    "\n",
    "# Simulate replica placement for each identified data\n",
    "for label in set(identified_data_labels):\n",
    "    # Extract data related to the identified cluster\n",
    "    cluster_data = replica_placement_data[identified_data_labels == label]\n",
    "\n",
    "    # Get placement decision from the neuro-fuzzy inference system\n",
    "    fuzzy_decision = nfis.fuzzy_inference(cluster_data)\n",
    "\n",
    "    # Interpret fuzzy decision based on placement rules\n",
    "    interpreted_decision = nfis.interpret_decision(fuzzy_decision[0])\n",
    "\n",
    "    placement_decisions.append(interpreted_decision)\n",
    "\n",
    "# Display placement decisions\n",
    "for i, decision in enumerate(placement_decisions):\n",
    "    print(f\"Placement Decision for Cluster {i+1}: {decision}\")\n",
    "\n",
    "# Adaptation mechanism (example: increase the number of data centers based on workload)\n",
    "if len(placement_decisions) > 5:\n",
    "    region1.data_centers.extend([DataCenter(f\"US_DC{i}\", region1) for i in range(num_dcs_max, num_dcs_max + 3)])\n",
    "\n",
    "# Evaluate replica placement performance\n",
    "accuracy = np.mean(np.array(placement_decisions) == np.array(identified_data_labels))\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_spectral.py:688: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\n",
      "c:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:273: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 1ms/step\n",
      "96/96 [==============================] - 0s 1ms/step\n",
      "102/102 [==============================] - 0s 1ms/step\n",
      "Placement Decision for Cluster 1: Very Low\n",
      "Placement Decision for Cluster 2: Very Low\n",
      "Placement Decision for Cluster 3: Very Low\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "class DataProvider:\n",
    "    def __init__(self, name, cpu_prices, storage_prices, bw_prices):\n",
    "        self.name = name\n",
    "        if len(cpu_prices) == len(storage_prices) == len(bw_prices):\n",
    "            self.cpu_prices = cpu_prices\n",
    "            self.storage_prices = storage_prices\n",
    "            self.bw_prices = bw_prices\n",
    "        else:\n",
    "            raise ValueError(\"Lengths of cpu_prices, storage_prices, and bw_prices must be the same.\")\n",
    "\n",
    "class Region:\n",
    "    def __init__(self, name, data_provider, num_dcs):\n",
    "        self.name = name\n",
    "        self.data_provider = data_provider\n",
    "        self.num_dcs = num_dcs\n",
    "        self.data_centers = [DataCenter(f\"{name}_DC{i}\", self) for i in range(num_dcs)]\n",
    "\n",
    "class DataCenter:\n",
    "    def __init__(self, name, region):\n",
    "        self.name = name\n",
    "        self.region = region\n",
    "        # Add other attributes like VMs, tasks, etc., as needed\n",
    "\n",
    "class NeuroFuzzyInferenceSystem:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(10, input_dim=self.data.shape[1], activation='relu'),\n",
    "            tf.keras.layers.Dense(5, activation='relu'),\n",
    "            tf.keras.layers.Dense(3, activation='softmax')  # Assuming 3 fuzzy sets (Low, Medium, High)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fuzzy_inference(self, input_data):\n",
    "        fuzzy_sets = self.model.predict(input_data)\n",
    "        # Fuzzy inference logic (centroid defuzzification)\n",
    "        fuzzy_decision = np.argmax(fuzzy_sets, axis=1)\n",
    "        return fuzzy_decision\n",
    "\n",
    "    def interpret_decision(self, fuzzy_decision):\n",
    "        # Interpret fuzzy decision based on the placement rules\n",
    "        if fuzzy_decision == 0:\n",
    "            return \"Very Low\"\n",
    "        elif fuzzy_decision == 1:\n",
    "            return \"Low\"\n",
    "        elif fuzzy_decision == 2:\n",
    "            return \"Medium\"\n",
    "        elif fuzzy_decision == 3:\n",
    "            return \"High\"\n",
    "        elif fuzzy_decision == 4:\n",
    "            return \"Very High\"\n",
    "\n",
    "# Simulation parameters\n",
    "num_providers = 3\n",
    "num_regions = 3\n",
    "num_dcs_min = 2\n",
    "num_dcs_max = 5\n",
    "num_vms_per_dc = 8\n",
    "num_tasks_min = 1000\n",
    "num_tasks_max = 10000\n",
    "task_size_min = 200\n",
    "task_size_max = 1000\n",
    "data_size_min = 300\n",
    "data_size_max = 1000\n",
    "inter_region_bw = 500  # Mb/s\n",
    "intra_region_bw = 1000  # Mb/s\n",
    "intra_dc_bw = 8000  # Mb/s\n",
    "vm_processing_capability = 1500  # MIPS\n",
    "vm_cpu = 2\n",
    "vm_ram = 4  # GB\n",
    "vm_storage_capacity = 8  # GB\n",
    "rev_per_task = 0.7  # $\n",
    "penalty_per_violation = 0.0025  # $\n",
    "response_time_slo = 180  # s\n",
    "availability_slo = 0.95\n",
    "replication_period = 32\n",
    "threshold_thrt = 0.8 * response_time_slo\n",
    "num_clusters = 3\n",
    "\n",
    "# Rule parameters\n",
    "data_transfer_time_ratio = [\"High\", \"High\", \"Medium\", \"Medium\", \"Low\"]\n",
    "vm_load = [\"High\", \"Medium\", \"Medium\", \"Medium\", \"Low\"]\n",
    "data_availability = [\"NR\", \"NR\", \"NR\", \"R\", \"R\"]\n",
    "provider_profit = [\"NP\", \"P\", \"P\", \"P\", \"P\"]\n",
    "placement_potential = [\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"]\n",
    "\n",
    "# Create data providers and regions\n",
    "provider1 = DataProvider(\"Provider1\", [0.020, 0.025, 0.027], [0.006, 0.006, 0.0066], [0.001, 0.0015, 0.002])\n",
    "provider2 = DataProvider(\"Provider2\", [0.020, 0.018, 0.020], [0.0096, 0.008, 0.0096], [0.001, 0.0015, 0.002])\n",
    "provider3 = DataProvider(\"Provider3\", [0.0095, 0.0090, 0.0080], [0.00120, 0.0096, 0.0090], [0.001, 0.0015, 0.002])\n",
    "\n",
    "region1 = Region(\"US\", provider1, num_dcs_max)\n",
    "region2 = Region(\"EU\", provider2, num_dcs_min)\n",
    "region3 = Region(\"AS\", provider3, num_dcs_max)\n",
    "\n",
    "# Data identification phase\n",
    "data_identification_data = np.random.rand(num_tasks_max, num_tasks_max)\n",
    "identified_data_labels = SpectralClustering(n_clusters=num_clusters, assign_labels=\"discretize\", random_state=0).fit_predict(data_identification_data)\n",
    "\n",
    "# Replica placement phase\n",
    "replica_placement_data = np.random.rand(num_tasks_max, 4)  # Assuming 4 input parameters for the neuro-fuzzy system\n",
    "nfis = NeuroFuzzyInferenceSystem(replica_placement_data)\n",
    "\n",
    "# Convert the identified data into one-hot encoded labels for training the neuro-fuzzy model\n",
    "identified_data_labels_onehot = tf.keras.utils.to_categorical(identified_data_labels, num_clusters)\n",
    "\n",
    "# Train the neuro-fuzzy model\n",
    "nfis.model.fit(replica_placement_data, identified_data_labels_onehot, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Placeholder for results\n",
    "placement_decisions = []\n",
    "\n",
    "# Simulate replica placement for each identified data\n",
    "for label in set(identified_data_labels):\n",
    "    # Extract data related to the identified cluster\n",
    "    cluster_data = replica_placement_data[identified_data_labels == label]\n",
    "\n",
    "    # Get placement decision from the neuro-fuzzy inference system\n",
    "    fuzzy_decision = nfis.fuzzy_inference(cluster_data)\n",
    "\n",
    "    # Interpret fuzzy decision based on placement rules\n",
    "    interpreted_decision = nfis.interpret_decision(fuzzy_decision[0])\n",
    "\n",
    "    placement_decisions.append(interpreted_decision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\YANSILIYU\\devProject\\fuzzy-SLA\\algorithm-3.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YANSILIYU/devProject/fuzzy-SLA/algorithm-3.ipynb#W4sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m correlation_potentials \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(num_tasks)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YANSILIYU/devProject/fuzzy-SLA/algorithm-3.ipynb#W4sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YANSILIYU/devProject/fuzzy-SLA/algorithm-3.ipynb#W4sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m# Perform spectral clustering in the data identification phase\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/YANSILIYU/devProject/fuzzy-SLA/algorithm-3.ipynb#W4sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m data_clusters \u001b[39m=\u001b[39m perform_spectral_clustering(data_sizes, num_clusters)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/YANSILIYU/devProject/fuzzy-SLA/algorithm-3.ipynb#W4sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39m# Perform neuro-fuzzy inference in the replica placement phase\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/YANSILIYU/devProject/fuzzy-SLA/algorithm-3.ipynb#W4sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m centers, _, _, _, _, _, _ \u001b[39m=\u001b[39m fuzz\u001b[39m.\u001b[39mcmeans(data_sizes, num_clusters, \u001b[39m2\u001b[39m, error\u001b[39m=\u001b[39m\u001b[39m0.005\u001b[39m, maxiter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\YANSILIYU\\devProject\\fuzzy-SLA\\algorithm-3.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YANSILIYU/devProject/fuzzy-SLA/algorithm-3.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_spectral_clustering\u001b[39m(data_sizes, num_clusters):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/YANSILIYU/devProject/fuzzy-SLA/algorithm-3.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     distances \u001b[39m=\u001b[39m pairwise_distances(data_sizes\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YANSILIYU/devProject/fuzzy-SLA/algorithm-3.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     spectral \u001b[39m=\u001b[39m SpectralClustering(n_clusters\u001b[39m=\u001b[39mnum_clusters, affinity\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest_neighbors\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YANSILIYU/devProject/fuzzy-SLA/algorithm-3.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     data_clusters \u001b[39m=\u001b[39m spectral\u001b[39m.\u001b[39mfit_predict(distances)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# Simulation configuration\n",
    "num_providers = 3\n",
    "num_regions = 3\n",
    "min_dcs_per_provider = 2\n",
    "max_dcs_per_provider = 5\n",
    "num_vms_per_dc = 8\n",
    "min_tasks = 1000\n",
    "max_tasks = 10000\n",
    "min_task_size = 200\n",
    "max_task_size = 1000\n",
    "data_sizes = 300\n",
    "max_data_size = 1000\n",
    "inter_region_bw = 500  # Mb/s\n",
    "intra_region_bw = 1000  # Mb/s\n",
    "intra_dc_bw = 8000  # Mb/s\n",
    "vm_processing_capability = 1500  # MIPS\n",
    "vm_num_cpu = 2\n",
    "vm_ram = 4  # Gb\n",
    "vm_storage_capacity = 8  # Gb\n",
    "provider_revenue_per_task = 0.7\n",
    "penalty_per_violation = 0.0025\n",
    "response_time_slo = 180  # seconds\n",
    "min_availability_slo = 0.95\n",
    "replication_period = 32  # violating tasks\n",
    "w = 0.8\n",
    "thrt = w * response_time_slo\n",
    "num_clusters = 3\n",
    "\n",
    "# Cloud Providers and Prices\n",
    "cpu_prices = np.array([[0.020, 0.025, 0.027],\n",
    "                       [0.020, 0.018, 0.020],\n",
    "                       [0.0095, 0.0090, 0.0080]])\n",
    "\n",
    "storage_prices = np.array([[0.006, 0.006, 0.0066],\n",
    "                           [0.0096, 0.008, 0.0096],\n",
    "                           [0.00120, 0.0096, 0.0090]])\n",
    "\n",
    "bw_prices = np.array([[0.001, 0.0015, 0.002],\n",
    "                      [0.001, 0.0015, 0.002],\n",
    "                      [0.001, 0.0015, 0.002]])\n",
    "\n",
    "# Rule parameters\n",
    "rules = np.array([[3, 3, 0, 0, 1],\n",
    "                  [3, 2, 0, 1, 2],\n",
    "                  [2, 2, 0, 1, 3],\n",
    "                  [2, 2, 1, 1, 4],\n",
    "                  [1, 1, 1, 1, 5]])\n",
    "\n",
    "# Data Identification Phase: Spectral Clustering\n",
    "def perform_spectral_clustering(data_sizes, num_clusters):\n",
    "    distances = pairwise_distances(data_sizes.reshape(-1, 1))\n",
    "    spectral = SpectralClustering(n_clusters=num_clusters, affinity='nearest_neighbors')\n",
    "    data_clusters = spectral.fit_predict(distances)\n",
    "    return data_clusters\n",
    "\n",
    "# Replica Placement Phase: Neuro-Fuzzy Inference System (using Fuzzy C-Means)\n",
    "def perform_neuro_fuzzy_inference(data_sizes, centers, num_clusters, rules, response_times, availability_scores,\n",
    "                                  leasing_costs, correlation_potentials):\n",
    "    # Assign each data point to the nearest cluster center\n",
    "    u, _, _, _, _, _, _ = fuzz.cmeans(data_sizes, num_clusters, 2, error=0.005, maxiter=1000, centers=centers)\n",
    "\n",
    "    # Calculate fuzzy memberships for each rule\n",
    "    rule_memberships = [fuzz.interp_membership(u, center, data_sizes) for center in centers]\n",
    "\n",
    "    # Calculate aggregated memberships based on rules\n",
    "    aggregated_memberships = [np.min([rule_memberships[i][j] for i in range(num_clusters)]) for j in range(len(data_sizes))]\n",
    "\n",
    "    # Calculate neuro-fuzzy inference scores for each resource\n",
    "    resource_scores = []\n",
    "    for i in range(num_clusters):\n",
    "        # Extract parameters from rules based on the identified data clusters\n",
    "        rule_params = rules[i]\n",
    "        data_correlation = aggregated_memberships\n",
    "        data_availability = availability_scores\n",
    "        leasing_cost = leasing_costs\n",
    "        response_time = response_times\n",
    "        correlation_potential = correlation_potentials\n",
    "\n",
    "        # Neuro-fuzzy inference formula (adjust as needed based on your actual system)\n",
    "        resource_score = rule_params[0] * data_correlation + rule_params[1] * data_availability + \\\n",
    "                         rule_params[2] * leasing_cost + rule_params[3] * response_time + \\\n",
    "                         rule_params[4] * correlation_potential\n",
    "\n",
    "        resource_scores.append(resource_score)\n",
    "\n",
    "    return resource_scores\n",
    "\n",
    "# Example calculations for availability scores, leasing costs, and correlation potentials\n",
    "num_tasks = 1000  # Assuming a value for the example\n",
    "availability_scores = np.random.rand(num_tasks)\n",
    "leasing_costs = np.random.rand(num_tasks)\n",
    "correlation_potentials = np.random.rand(num_tasks)\n",
    "# Example usage:\n",
    "# Perform spectral clustering in the data identification phase\n",
    "data_clusters = perform_spectral_clustering(data_sizes, num_clusters)\n",
    "\n",
    "# Perform neuro-fuzzy inference in the replica placement phase\n",
    "centers, _, _, _, _, _, _ = fuzz.cmeans(data_sizes, num_clusters, 2, error=0.005, maxiter=1000)\n",
    "resource_scores = perform_neuro_fuzzy_inference(data_sizes, centers, num_clusters, selected_rules, response_times,\n",
    "                                                availability_scores, leasing_costs, correlation_potentials)\n",
    "\n",
    "def simulate_dc_variation(min_dcs, max_dcs, num_clusters, rules, cpu_prices, storage_prices, bw_prices):\n",
    "    results = []\n",
    "    for num_dcs in range(min_dcs, max_dcs + 1):\n",
    "        # Generate simulation data (you can replace this with your actual data generation logic)\n",
    "        task_sizes = np.random.randint(min_task_size, max_task_size + 1, num_tasks)\n",
    "        data_sizes = np.random.randint(min_data_size, max_data_size + 1, num_tasks)\n",
    "\n",
    "        # Data Identification Phase: Spectral Clustering\n",
    "        distances = pairwise_distances(data_sizes.reshape(-1, 1))\n",
    "        spectral = SpectralClustering(n_clusters=num_clusters, affinity='nearest_neighbors')\n",
    "        data_clusters = spectral.fit_predict(distances)\n",
    "\n",
    "        # Replica Placement Phase: Neuro-Fuzzy Inference System (using Fuzzy C-Means)\n",
    "        centers, u, _, _, _, _, _ = fuzz.cmeans(data_sizes, num_clusters, 2, error=0.005, maxiter=1000)\n",
    "\n",
    "        # Extract relevant parameters from the rules based on the identified data clusters\n",
    "        rule_indices = data_clusters\n",
    "        selected_rules = rules[rule_indices]\n",
    "\n",
    "        # Evaluate the performance based on your strategy...\n",
    "        # Your simulation and evaluation logic here...\n",
    "\n",
    "        # Placeholder: Calculate average response time for each task\n",
    "        response_times = [calculate_response_time(task_sizes[i], data_sizes[i], vm_processing_capability, intra_dc_bw)\n",
    "                           for i in range(num_tasks)]\n",
    "        average_response_time = sum(response_times) / num_tasks\n",
    "\n",
    "        results.append((num_dcs, average_response_time))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "dc_variation_results = simulate_dc_variation(min_dcs_per_provider, max_dcs_per_provider, num_clusters, rules,\n",
    "                                            cpu_prices, storage_prices, bw_prices)\n",
    "\n",
    "# Print or use the results as needed...\n",
    "print(\"Average Response Time:\", response_times)\n",
    "print(\"SLA Violations:\", sla_violations)\n",
    "print(\"Effective Network Usage:\", total_data_transfer)\n",
    "print(\"Average Monetary Profit per Provider:\", average_revenues_per_provider)\n",
    "print(\"Average Response Time while varying tasks number:\", task_variation_results)\n",
    "print(\"Average Response Time while varying data centers number:\", dc_variation_results)\n",
    "\n",
    "# For example, you can print the resource scores for each data point:\n",
    "print(\"Resource Scores:\", resource_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
