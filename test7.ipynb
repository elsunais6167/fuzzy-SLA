{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpy\n",
    "import random\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VirtualMachine:\n",
    "    def __init__(self, env, id, processing_capability, num_cpus, ram, storage_capacity, data_center, region):\n",
    "        self.env = env\n",
    "        self.id = id\n",
    "        self.processing_capability = processing_capability\n",
    "        self.num_cpus = num_cpus\n",
    "        self.ram = ram\n",
    "        self.storage_capacity = storage_capacity\n",
    "        self.data_center = data_center\n",
    "        self.region = region\n",
    "        self.uptime = 0\n",
    "        self.total_time = 0\n",
    "        self.data_access_log = {}\n",
    "    \n",
    "    def update_availability(self, time_passed):\n",
    "        # Call this method periodically to update uptime and total_time\n",
    "        self.total_time += time_passed  # Update with the actual time passed\n",
    "        # Uptime should be updated based on whether the VM was available during this period\n",
    "\n",
    "    def get_availability(self):\n",
    "        if self.total_time == 0:\n",
    "            return 1  # Assuming always available when no time has passed\n",
    "        return self.uptime / self.total_time\n",
    "    \n",
    "    def process_task(self, task, target_dc, data_transfer_time=0):\n",
    "        start_time = self.env.now\n",
    "        total_processing_time = task['size'] / self.processing_capability + data_transfer_time\n",
    "        yield self.env.timeout(total_processing_time)\n",
    "        end_time = self.env.now\n",
    "\n",
    "        # Calculate response time and check for SLA violation or near violation\n",
    "        response_time = end_time - start_time\n",
    "        sla_rt = 180  # SLORT in seconds\n",
    "        sla_ma = 0.95  # SLOMA in seconds\n",
    "        w = 0.8  # Define the weight factor\n",
    "        th_rt = w * sla_rt  # Threshold for near SLA violation\n",
    "\n",
    "        penalty = 0\n",
    "        near_violation = False\n",
    "        if response_time > sla_rt:\n",
    "            penalty = 0.0025  # Penalty per violation\n",
    "        elif response_time > th_rt:\n",
    "            near_violation = True  # Task is near to causing SLA violation\n",
    "\n",
    "        cost = self.data_center.calculate_cost(task['size'], target_dc) - penalty\n",
    "        self.data_center.total_revenue += (0.7 - penalty)  # Update total revenue for the data center\n",
    "        self.data_center.near_violations += int(near_violation)  # Update near violations count for the data center\n",
    "\n",
    "        self.data_access_log[task['id']] = {\n",
    "            'response_time': response_time,\n",
    "            'penalty': penalty,\n",
    "            'near_violation': near_violation\n",
    "        }\n",
    "    \n",
    "    def calculate_correlation_matrix(self, data_accessed, P):\n",
    "        n = len(data_accessed)\n",
    "        correlation_matrix = np.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    # T_k(di) is the set of tasks accessing data di\n",
    "                    T_k_di = data_accessed[i]['tasks']\n",
    "                    T_k_dj = data_accessed[j]['tasks']\n",
    "                    common_tasks = len(set(T_k_di).intersection(set(T_k_dj)))\n",
    "                    correlation_matrix[i, j] = common_tasks / P\n",
    "                else:\n",
    "                    correlation_matrix[i, j] = 0\n",
    "        return correlation_matrix\n",
    "\n",
    "    def perform_spectral_clustering(self, correlation_matrix, K):\n",
    "        # Apply spectral clustering\n",
    "        sc = SpectralClustering(n_clusters=K, affinity='precomputed')\n",
    "        group_labels = sc.fit_predict(correlation_matrix)\n",
    "        return group_labels\n",
    "\n",
    "    def select_data_to_replicate(self, data_accessed, group_labels):\n",
    "        # Calculate average access frequency for each group\n",
    "        available_vms = [vm for vm in self.data_center.vms if vm.get_availability() >= self.slo_ma]\n",
    "        unique_labels = set(group_labels)\n",
    "        groups = {label: [] for label in unique_labels}\n",
    "\n",
    "        for i, label in enumerate(group_labels):\n",
    "            groups[label].append(data_accessed[i])\n",
    "\n",
    "        data_to_replicate = []\n",
    "\n",
    "        for label, group in groups.items():\n",
    "            avg_freq = sum(d['access_frequency'] for d in group) / len(group)\n",
    "            data_to_replicate.extend([d for d in group if d['access_frequency'] >= avg_freq])\n",
    "            data_to_replicate = [d for d in data_to_replicate if self.is_in_same_region(d)]\n",
    "\n",
    "        return data_to_replicate\n",
    "    \n",
    "    def is_in_same_region(self, data_item):\n",
    "        # Logic to check if the data item is frequently accessed within the same region\n",
    "        return all(vm.region == self.region for vm in data_item['accessing_vms'])\n",
    "    \n",
    "    # Update the adjust_replicas method to include the new strategy\n",
    "    def adjust_replicas(self, P, K):\n",
    "        # This list is populated with actual data accessed information\n",
    "        data_accessed = [{'id': d, 'access_frequency': self.data_access_log[d]['access_frequency'], 'tasks': self.data_access_log[d]['tasks']} for d in self.data_access_log]\n",
    "        \n",
    "        correlation_matrix = self.calculate_correlation_matrix(data_accessed, P)\n",
    "        group_labels = self.perform_spectral_clustering(correlation_matrix, K)\n",
    "        data_to_replicate = self.select_data_to_replicate(data_accessed, group_labels)\n",
    "\n",
    "        for data_item in data_to_replicate:\n",
    "            # Find the best VM considering both response time and availability\n",
    "            best_vm = self.find_best_vm_for_replication(data_item)\n",
    "    \n",
    "    def find_best_vm_for_replication(self, data_item):\n",
    "        # Filter VMs based on storage capacity\n",
    "        suitable_vms = [vm for vm in self.data_center.vms if vm.storage_capacity >= data_item['size'] and vm.get_availability() >= self.slo_ma]\n",
    "\n",
    "        # Prioritize VMs in the same data center or region to minimize bandwidth costs and transfer time\n",
    "        same_dc_vms = [vm for vm in suitable_vms if vm.data_center.id == self.data_center.id]\n",
    "        same_region_vms = [vm for vm in suitable_vms if vm.region == self.region and vm not in same_dc_vms]\n",
    "\n",
    "        # Select the best VM based on the lowest cost, prioritizing same DC, then same region\n",
    "        best_vm = None\n",
    "        if same_dc_vms:\n",
    "            best_vm = min(same_dc_vms, key=lambda vm: self.data_center.calculate_cost(data_item['size'], vm.data_center))\n",
    "        elif same_region_vms:\n",
    "            best_vm = min(same_region_vms, key=lambda vm: self.data_center.calculate_cost(data_item['size'], vm.data_center))\n",
    "        else:\n",
    "            best_vm = min(suitable_vms, key=lambda vm: self.data_center.calculate_cost(data_item['size'], vm.data_center)) if suitable_vms else None\n",
    "\n",
    "        return best_vm\n",
    "\n",
    "class DataCenter:\n",
    "    def __init__(self, env, id, num_vms, vm_specs, pricing, bw_params, region):\n",
    "        self.env = env\n",
    "        self.id = id\n",
    "        self.vm_specs = vm_specs\n",
    "        self.pricing = pricing\n",
    "        self.bw_params = bw_params\n",
    "        self.total_revenue = 0\n",
    "        self.near_violations = 0\n",
    "        self.region = region\n",
    "        self.vms = [VirtualMachine(env, f\"VM_{id}_{i}\", **vm_specs, data_center=self, region=self.region) for i in range(num_vms)]\n",
    "    \n",
    "    def get_total_revenue(self):\n",
    "        return self.total_revenue\n",
    "    \n",
    "    def get_near_violations_count(self):\n",
    "        return self.near_violations\n",
    "    \n",
    "    def calculate_cost(self, task_size, target_dc):\n",
    "        cpu_cost = task_size / 107 * self.pricing['cpu_price']\n",
    "        storage_cost = self.vm_specs['storage_capacity'] * self.pricing['storage_price']\n",
    "\n",
    "        if self.id == target_dc.id:\n",
    "            # Intra-DC transfer\n",
    "            bw_cost = task_size * self.pricing['bw_price']['intra_dc']\n",
    "        elif self.id.split('_')[1] == target_dc.id.split('_')[1]:\n",
    "            # Intra-region transfer\n",
    "            bw_cost = task_size * self.pricing['bw_price']['intra_region']\n",
    "        else:\n",
    "            # Inter-region transfer\n",
    "            bw_cost = task_size * self.pricing['bw_price']['inter_region']\n",
    "\n",
    "        return cpu_cost + storage_cost + bw_cost\n",
    "\n",
    "    def calculate_data_transfer_time(self, task_size, target_dc):\n",
    "        if self.id == target_dc.id:\n",
    "            # Intra-DC transfer\n",
    "            bw_capacity = self.bw_params['intra_dc']['capacity']\n",
    "            bw_delay = self.bw_params['intra_dc']['delay']\n",
    "        elif self.id.split('_')[1] == target_dc.id.split('_')[1]:\n",
    "            # Intra-region transfer\n",
    "            bw_capacity = self.bw_params['intra_region']['capacity']\n",
    "            bw_delay = self.bw_params['intra_region']['delay']\n",
    "        else:\n",
    "            # Inter-region transfer\n",
    "            bw_capacity = self.bw_params['inter_region']['capacity']\n",
    "            bw_delay = self.bw_params['inter_region']['delay']\n",
    "\n",
    "        data_transfer_time = task_size / bw_capacity + bw_delay / 1000  # Convert delay from ms to seconds\n",
    "        return data_transfer_time\n",
    "\n",
    "class CloudProvider:\n",
    "    def __init__(self, env, id, num_regions, dcs_per_region, vm_specs, pricing, bw_params):\n",
    "        self.env = env\n",
    "        self.id = id\n",
    "        self.pricing = pricing\n",
    "        self.bw_params = bw_params\n",
    "        region_keys = list(pricing.keys())  # Get the region keys from pricing\n",
    "        self.regions = [DataCenter(env, f\"DC_{id}_{region_keys[r]}\", random.randint(*dcs_per_region), vm_specs, pricing[region_keys[r]], bw_params, region=region_keys[r]) for r in range(num_regions)]\n",
    "    \n",
    "    def find_target_dc(self, task):\n",
    "        # Simplified logic: Randomly select a data center\n",
    "        return random.choice(self.regions)\n",
    "\n",
    "# Bandwidth parameters\n",
    "bw_params = {\n",
    "    'inter_region': {'capacity': 500, 'delay': 150},\n",
    "    'intra_region': {'capacity': 1000, 'delay': 50},\n",
    "    'intra_dc': {'capacity': 8000, 'delay': 10}\n",
    "}\n",
    "\n",
    "# Simulation parameters\n",
    "num_providers = 3\n",
    "num_regions = 3\n",
    "dcs_per_provider = (2, 5)\n",
    "num_vms_per_dc = 8\n",
    "vm_specs = {\n",
    "    'processing_capability': 1500,  # in MIPS\n",
    "    'num_cpus': 2,\n",
    "    'ram': 4,  # in Gb\n",
    "    'storage_capacity': 8  # in Gb\n",
    "}\n",
    "num_data = 200\n",
    "task_size_range = (200, 1000)  # Task size range in MI\n",
    "\n",
    "# Pricing structure\n",
    "pricing = {\n",
    "    'Provider_1': {\n",
    "        'US': {'cpu_price': 0.020, 'storage_price': 0.006, 'bw_price': {'intra_dc': 0.001, 'inter_region': 0.008}},\n",
    "        'EU': {'cpu_price': 0.025, 'storage_price': 0.006, 'bw_price': {'intra_dc': 0.0015, 'inter_region': 0.008}},\n",
    "        'AS': {'cpu_price': 0.027, 'storage_price': 0.0066, 'bw_price': {'intra_dc': 0.002, 'inter_region': 0.008}}\n",
    "    },\n",
    "    'Provider_2': {\n",
    "        'US': {'cpu_price': 0.020, 'storage_price': 0.0096, 'bw_price': {'intra_dc': 0.001, 'inter_region': 0.008}},\n",
    "        'EU': {'cpu_price': 0.018, 'storage_price': 0.008, 'bw_price': {'intra_dc': 0.0015, 'inter_region': 0.008}},\n",
    "        'AS': {'cpu_price': 0.020, 'storage_price': 0.0096, 'bw_price': {'intra_dc': 0.002, 'inter_region': 0.008}}\n",
    "    },\n",
    "    'Provider_3': {\n",
    "        'US': {'cpu_price': 0.0095, 'storage_price': 0.0012, 'bw_price': {'intra_dc': 0.001, 'inter_region': 0.008}},\n",
    "        'EU': {'cpu_price': 0.0090, 'storage_price': 0.0096, 'bw_price': {'intra_dc': 0.0015, 'inter_region': 0.008}},\n",
    "        'AS': {'cpu_price': 0.0080, 'storage_price': 0.0090, 'bw_price': {'intra_dc': 0.002, 'inter_region': 0.008}}\n",
    "    }\n",
    "}\n",
    "\n",
    "env = simpy.Environment()\n",
    "P = 32 # Number of violating tasks\n",
    "K = 3 #Number of Cluster\n",
    "\n",
    "providers = []\n",
    "for i in range(num_providers):\n",
    "    provider_key = f'Provider_{i+1}'  # This should match the keys in the pricing dictionary\n",
    "    if provider_key in pricing:\n",
    "        provider = CloudProvider(env, provider_key, num_regions, dcs_per_provider, vm_specs, pricing[provider_key], bw_params)\n",
    "        providers.append(provider)\n",
    "    else:\n",
    "        print(f\"Warning: Pricing not found for {provider_key}\")\n",
    "        \n",
    "tasks = [{'id': i, 'size': random.randint(*task_size_range)} for i in range(num_data)]\n",
    "\n",
    "for task in tasks:\n",
    "    # Example logic for selecting source and target data centers with geographic considerations\n",
    "    random_provider = random.choice(providers)\n",
    "    source_dc = random.choice(random_provider.regions)\n",
    "    target_dc = random.choice(random_provider.regions)\n",
    "\n",
    "    # Include logic to prefer VMs in the same region if possible\n",
    "    preferred_vms = [vm for vm in target_dc.vms if vm.region == source_dc.region]\n",
    "    random_vm = random.choice(preferred_vms if preferred_vms else target_dc.vms)\n",
    "\n",
    "    data_transfer_time = source_dc.calculate_data_transfer_time(task['size'], target_dc)\n",
    "    env.process(random_vm.process_task(task, target_dc, data_transfer_time))\n",
    "\n",
    "env.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
