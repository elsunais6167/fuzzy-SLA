{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YANSILIYU\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:273: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 138\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProfit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total_revenue \u001b[38;5;241m-\u001b[39m total_cost \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-Profit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m critical_data:\n\u001b[1;32m--> 138\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_transfer_time_category\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m categorize_data_transfer_time(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponse_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    139\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvm_load_category\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m categorize_vm_load(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_count\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    140\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_availability_category\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m categorize_data_availability(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavailability\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulation parameters\n",
    "num_providers = 3\n",
    "num_regions = 3\n",
    "min_dcs_per_provider = 2\n",
    "max_dcs_per_provider = 5\n",
    "num_vms_per_dc = 8\n",
    "vm_processing_capability = 1500  # in MIPS\n",
    "vm_num_cpus = 2\n",
    "vm_ram = 4  # in Gb\n",
    "vm_storage_capacity = 8  # in Gb\n",
    "num_data = 200  # Number of data\n",
    "data_size_range = (300, 1000)  # Data size range in Mb\n",
    "task_size_range = (200, 1000)  # Task size range in MI\n",
    "Re = 0.7  # Provider revenues per task execution ($)\n",
    "C_penalty = 0.0025  # Penalty per violation ($)\n",
    "num_dc_list = [6, 9, 12, 15]\n",
    "\n",
    "inter_region_bw_capacity = 500  # in Mb/s\n",
    "inter_region_bw_delay = 150  # in ms\n",
    "\n",
    "intra_region_bw_capacity = 1000  # in Mb/s\n",
    "intra_region_bw_delay = 50  # in ms\n",
    "\n",
    "intra_dc_bw_capacity = 8000  # in Mb/s\n",
    "intra_dc_bw_delay = 10  # in ms\n",
    "\n",
    "# Pricing information\n",
    "operating_cost = {\n",
    "    'Provider 1': {\n",
    "        'US': [0.020, 0.006, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.025, 0.006, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.027, 0.0066, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "    'Provider 2': {\n",
    "        'US': [0.020, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.018, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.020, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "    'Provider 3': {\n",
    "        'US': [0.0095, 0.00120, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'EU': [0.0090, 0.0096, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "        'AS': [0.0080, 0.0090, 0.001, 0.0015, 0.002, 0.004, 0.008],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Simulate data for each provider and region\n",
    "for num_dcs in num_dc_list:\n",
    "    provider_data = []  # Reset provider data for each iteration\n",
    "    for provider_id in range(1, num_providers + 1):\n",
    "        for region in ['US', 'EU', 'AS']:\n",
    "            for dc_id in range(1, num_dcs + 1):                \n",
    "                for data_id in range(1, num_data + 1):\n",
    "                    data_size = np.random.uniform(data_size_range[0], data_size_range[1])\n",
    "                    task_size = np.random.uniform(task_size_range[0], task_size_range[1])\n",
    "                    \n",
    "                    base_response_time = 180\n",
    "                    response_time_variation = np.random.normal(loc=0, scale=10)\n",
    "                    response_time = max(0, base_response_time + response_time_variation)\n",
    "                    \n",
    "                    # Include bandwidth and delay information in the data\n",
    "                    vm_data = {\n",
    "                        'provider_id': provider_id,\n",
    "                        'region': region,\n",
    "                        'dc_id': dc_id,\n",
    "                        'num_vms': num_vms_per_dc,\n",
    "                        'vm_processing_capability': vm_processing_capability,\n",
    "                        'vm_num_cpus': vm_num_cpus,\n",
    "                        'vm_ram': vm_ram,\n",
    "                        'vm_storage_capacity': vm_storage_capacity,\n",
    "                        'cpu_cost': operating_cost[f'Provider {provider_id}'][region][0],\n",
    "                        'storage_cost': operating_cost[f'Provider {provider_id}'][region][1],\n",
    "                        'intra_dc_bw_cost': operating_cost[f'Provider {provider_id}'][region][2],\n",
    "                        'inter_region_bw_cost': operating_cost[f'Provider {provider_id}'][region][3],\n",
    "                        'intra_region_bw_cost': operating_cost[f'Provider {provider_id}'][region][4],\n",
    "                        'response_time_slo': response_time,\n",
    "                        'availability_slo': 0.95,\n",
    "                        'task_count': np.random.choice([1000, 2000, 3000, 5000, 7000, 10000]),\n",
    "                        'data_size': data_size,\n",
    "                        'task_size': task_size,\n",
    "                        'inter_region_bw_capacity': inter_region_bw_capacity,\n",
    "                        'inter_region_bw_delay': inter_region_bw_delay,\n",
    "                        'intra_region_bw_capacity': intra_region_bw_capacity,\n",
    "                        'intra_region_bw_delay': intra_region_bw_delay,\n",
    "                        'intra_dc_bw_capacity': intra_dc_bw_capacity,\n",
    "                        'intra_dc_bw_delay': intra_dc_bw_delay,\n",
    "                    }\n",
    "                    provider_data.append(vm_data)\n",
    "relevant_features = [\n",
    "    'response_time_slo', 'availability_slo', 'cpu_cost', 'storage_cost',\n",
    "    'intra_dc_bw_cost', 'inter_region_bw_cost', 'intra_region_bw_cost',\n",
    "    'task_count', 'data_size', 'task_size'\n",
    "]\n",
    "\n",
    "cluster_data = np.array([[vm.get(feature, 0) for feature in relevant_features] for vm in provider_data])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "\n",
    "# Step 2: Spectral Clustering\n",
    "spectral = SpectralClustering(n_clusters=3, affinity='nearest_neighbors', n_neighbors=5)\n",
    "cluster_labels = spectral.fit_predict(cluster_data_scaled)\n",
    "\n",
    "# Cluster Analysis\n",
    "cluster_density = np.bincount(cluster_labels)\n",
    "most_critical_clusters = np.argsort(cluster_density)[-2:]\n",
    "critical_data_indices = np.hstack([np.where(cluster_labels == cluster)[0] for cluster in most_critical_clusters])\n",
    "critical_data = cluster_data[critical_data_indices]\n",
    "\n",
    "\n",
    "# Placeholder for fuzzy logic categorization functions\n",
    "def categorize_data_transfer_time(response_time):\n",
    "    # Placeholder for categorizing data transfer time\n",
    "    return \"high\" if response_time > 200 else \"medium\" if response_time > 150 else \"low\"\n",
    "\n",
    "def categorize_vm_load(task_count):\n",
    "    # Placeholder for categorizing VM load\n",
    "    return \"high\" if task_count > 7000 else \"medium\" if task_count > 3000 else \"low\"\n",
    "\n",
    "def categorize_data_availability(availability):\n",
    "    # Placeholder for categorizing data availability\n",
    "    return \"Respected\" if availability > 0.95 else \"Non-respected\"\n",
    "\n",
    "def categorize_provider_profit(cpu_price, storage_price, total_revenue):\n",
    "    # Placeholder for categorizing provider profit\n",
    "    total_cost = cpu_price + storage_price  # Simplified cost calculation\n",
    "    return \"Profit\" if total_revenue - total_cost > 0 else \"Non-Profit\"\n",
    "\n",
    "for data in critical_data:\n",
    "    data['data_transfer_time_category'] = categorize_data_transfer_time(data['response_time'])\n",
    "    data['vm_load_category'] = categorize_vm_load(data['task_count'])\n",
    "    data['data_availability_category'] = categorize_data_availability(data['availability'])\n",
    "    # Assuming 'total_revenue' is a placeholder for revenue generated from this resource\n",
    "    data['provider_profit_category'] = categorize_provider_profit(data['cpu_price'], data['storage_price'], total_revenue)\n",
    "\n",
    "categorized_data = [{k: data[k] for k in ['data_transfer_time_category', 'vm_load_category', 'data_availability_category', 'provider_profit_category']} for data in critical_data]\n",
    "print(categorized_data[:5]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
